{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea3026",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain\n",
    "%pip install -U langchain-core\n",
    "%pip install -U langchain-community\n",
    "%pip install -U langchain-text-splitters\n",
    "%pip install -U pypdf\n",
    "%pip install -U pymupdf\n",
    "%pip install -U faiss-cpu\n",
    "%pip install -U chromadb\n",
    "%pip install -U sentence-transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b004d872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from typing import Any, List\n",
    "from sqlite3 import connect\n",
    "from datetime import datetime \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da719ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs\n",
    "\n",
    "class EmbeddingManager:\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        try:\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "    \n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        return embeddings\n",
    "\n",
    "class VectorStore:\n",
    "    def __init__(self, collection_name: str, persist_directory: str):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        try:\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\n",
    "                    \"description\": \"Document embeddings for RAG\"\n",
    "                }\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "class RAGRetriever:    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0):\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a739199",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_manager=EmbeddingManager()\n",
    "vectorstore = VectorStore('txt_messages', '../Data/vector_store')\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7a1622",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = os.getenv('DATA')\n",
    "\n",
    "sms = pd.read_csv(loc+\"sms.csv\", header=None)\n",
    "sms = sms.loc[:, [2, 4, 5, 9, 12, 18]]\n",
    "sms.columns = [\"Address\", \"Date Sent\", \"Date Received\", \"Type\", \"Body\", \"Seen\"]\n",
    "for i in sms[1:]:\n",
    "    sms[i] = sms[i].transform(lambda x: x.split(\"=\")[1])\n",
    "\n",
    "sms[\"Address\"] = sms[\"Address\"].replace(regex=\"^\\\\+1\", value=\"\")\n",
    "sms[\"Date Sent\"] = sms[\"Date Sent\"].transform(lambda x: datetime.fromtimestamp(int(x)//1000).strftime(\"%d-%m-%Y %H:%M:%S\"))\n",
    "sms[\"Type\"] = sms[\"Type\"].transform(lambda x: \"Received\" if x == \"1\" else \"Sent\")\n",
    "sms[\"Seen\"] = sms[\"Seen\"].transform(lambda x: \"True\" if x == \"1\" else \"False\")\n",
    "    \n",
    "for i in sms.index:\n",
    "    sms.at[i, \"Date Received\"] = sms.at[i, \"Date Sent\"] if sms.at[i, \"Type\"] == \"Sent\" else datetime.fromtimestamp(int(sms.at[i, \"Date Received\"])//1000).strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "        \n",
    "sms_documents = []\n",
    "for i in sms.index:\n",
    "    sms_documents.append(\n",
    "        Document(\n",
    "            page_content=sms.at[i, \"Body\"],\n",
    "            metadata={\n",
    "                \"Sender\": sms.at[i, \"Address\"],\n",
    "                \"Date Time Sent\": sms.at[i, \"Date Sent\"],\n",
    "                \"Date Time Received\": sms.at[i, \"Date Received\"],\n",
    "                \"Type\": sms.at[i, \"Type\"],\n",
    "                \"Seen\": sms.at[i, \"Seen\"]\n",
    "            }\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc39dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = split_documents(sms_documents)\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "vectorstore.add_documents(sms_documents, embeddings)\n",
    "rag_retriever.retrieve(\"threat\", score_threshold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore=VectorStore(collection_name=\"txt_messages\", persist_directory=\"../data/vector_store/\")\n",
    "vectorstore=VectorStore(collection_name=\"contacts\", persist_directory=\"../data/vector_store/\")\n",
    "vectorstore=VectorStore(collection_name=\"call_logs\", persist_directory=\"../data/vector_store/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00856e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in embeddings:\n",
    "    print(len(e))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
